badOzone = data["Ozone" = NA]
badOzone = data["Ozone"]
badOzone = is.na(data["Ozone"])
data[5,]
badOzone
length(badOzone[badOzone==FALSE])
length(badOzone[badOzone==TRUE])
mean(badOzone[badOzone==FALSE])
mean(badOzone)
mean(data[Ozone])
mean(data["Ozone"])
data["Ozone"]
mean(data)
mean(data[Ozone])
mean(data[$Ozone])
data[1]
mean(data[1])
help(mean)
numdata=as.numeric(data)
numdata=as.numeric(data[1])
numdata <- as.numeric(data[1])
ozone <- data[1]
as.numeric(ozone)
mean(ozone)
numdata <- as.numeric(levels(data))[data]
numdata <- as.numeric(levels(data))["Ozone"]
data[1]
data[$Oz]
data["Ozone"]
class(data)
data["Ozone"]
mean(data["Ozone"])
numozone <- as.numeric(data["Ozone"])
colmeans(data, na.rm=TRUE)
colMeans(data, na.rm=TRUE)
subdata <- data["Ozone" > 31]
subdata
subdata <- data[data[1] > 31]
class(subdata)
gooddata <- complete.cases(data)
data[gooddata, ][1:6, ]
data[gooddata, ]
data[gooddata, ][1:6, ]
data[gooddata, ][1:6, ]["Ozone" > 30]
data[gooddata, ]["Ozone" > 30, ]
data[ozone]
data["Ozone"]
data[gooddata, ][data["Ozone"] > 30, ]
data[data["Ozone"] > 30, ]
[data["Ozone"] > 30, ]
data["Ozone"] > 30
data[c(1,3)]
data[1>30]
data[1>30, ]
data[data[1]>30, ]
data[data[1]>30, ][gooddata, ]
good <- gooddata
gooddata <- data[good, ]
gooddata
gooddata[gooddata[1]>30, ]
gooddata[gooddata["Ozone"]>30, ]
gooddata[gooddata["Ozone"]>31, ][gooddata["Temp"]>90, ]
subdata <- gooddata[gooddata["Ozone"]>30, ]
subdata <- gooddata[gooddata["Ozone"]>31, ]
subdata <- subdata[subdata["Temp"]>90, ]
subdata
colmeans(subdata, na.rm=TRUE)
colMeans(subdata, na.rm=TRUE)
subdata2 <- data[data["Month"]=6, ]
subdata2 <- data[data["Month"]==6, ]
goodsub2 <- is.na(subdata2)
subdata2[goodsub2, ]
subdata2[!goodsub2, ]
subdata2
colMeans(subdata2, na.rm=FALSE)
colMeans(subdata2, na.rm=TRUE)
data[data["Month"]==5]
data[data["Month"]==5, ]
month5 <- data[data["Month"]==5, ]
help(max)
hep(colmax)
help(colmax)
help(colMax)
max(help["Ozone"])
max(month5["Ozone"])
max(month5["Ozone"], na.rm = TRUE)
add2 <- function(x, y) {
x + y
}
add2(2,3)
above <- function(x, n) {
use <- x > n
x[use]
}
x <- 1:20
above(x,12)
above <- function(x, n = 10) {
use <- x > n
x[use]
}
above(x)
columnmean <- function(y) {
nc <- ncol(y)
means <- numeric(nc)
for(i in 1:nc) {
means[i] <- mean(y[, i])
}
means # this gets returned
}
search()
help(gl)
install.packages("swirl")
library(swirl)
swirl()
5+7
x <- 5 + 7
x
y <- x - 3
y
z <- c(1.1, 9, 3.14)
?c
z
c(z, 555, z)
z * 2 + 100
my_sqrt <- sqrt(z - 1)
my_sqrt
my_div <- z / my_sqrt
my_div
c(1,2,3,4) + c(0,10)
c(1,2,3,4) + c(0,10,100)
z * 2 + 1000
my_div
mean(x)
traceback()
x <- 1
traceback()
library(datasets)
data(iris)
?iris
summary(iris)
head(iris)
mean(iris[$Species == virginica,])
mean(iris[iris$Species == virginica,])
str(iris)
iris$Species
iris(which(iris$Species == "virginica"), )
iris[which(iris$Species == "virginica"), ]
mean(iris[which(iris$Species == "virginica"), ])
vdat <- iris[which(iris$Species == "virginica"), ]
View(vdat)
mean(vdat$Species)
mean(vdat$Sepal.Length)
rowMeans(iris[, 1:4])
apply(iris[,1:4],2,mean)
apply(iris[,1:4],1,mean)
apply(iris,1,mean)
library(datasets)
data(mtcars)
?mtcars
str(mtcars)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
sapply(split(mtcars$hp, mtcars$cyl), mean)
hpower <- sapply(split(mtcars$hp, mtcars$cyl), mean)
hpower[,3]
hpower
str(hpower)
hpower(3) - hpower(2)
hpower[3]-hpower[2]
hpower[3]-hpower[1]
debug(ls)
ls
ls()
q
?browse
?clear
str(x)
str(z)
str(str)
set.seed(1)
rpois(5, 2)
set.seed(10)
x <- rbinom(10, 10, 0.5)
e <- rnorm(10, 0, 20)
y <- 0.5 + 2 * x + e
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) +geom_smooth
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies, panel = panel.loess)
qplot(rating, votes, data = movies) + geom_smooth()
install.packages("knitr")
library("knitr")
?lead
?lead()
library(Lahman)
install.packages("Lahman")
library(Lahman)
?lead
?lead()
?lag
source('~/.active-rstudio-document', echo=TRUE)
# Chain of Rocks Lock - Wood River
df <- getTrafficData("MI", "27", 10)
# Print table
df[,c(8, 11, 13)]
daySum <- ddply(.data = df,
.variables = .(day = as.Date(END_OF_LOCKAGE,
format = "%Y-%m-%d")),
summarize,
totalBarges = sum(as.numeric(NUM_BARGES)),
totalBbls = sum(bbls))
# Make some plots
qplot(x = day, y = totalBarges, data = daySum, stat = 'identity',
geom = 'bar', xlab = "Date", ylab = "Number of Barges",
main = "Wood River Traffic")
qplot(x = day, y = totalBbls/1000, data = daySum, stat = 'identity',
geom = 'line', xlab = "Date", ylab = "Barrels (in thousands)",
main = "Wood River Flow") + geom_smooth()
View(df)
x <- c(13,14,16,20,23,19,7)
x
write.csv(x, file = "")
write.csv(x, file = "")[2]
x
paste(x, collapse = ",")
xString = paste(x, collapse = ",")
source('~/.active-rstudio-document', echo=TRUE)
df <- manyToOneTrip(location1 = c(13, 14, 16, 20, 23, 7), location2 = (46), startDate = '2015-02-01',
minTravelTime = 2, maxTravelTime = 100)
df <- manyToOneTrip(location1 = c(13, 14, 16, 20, 23, 7), location2 = (46), startDate = '2015-02-01',
minTravelTime = 2, maxTravelTime = 100)
source('~/.active-rstudio-document', echo=TRUE)
install.packages("RPostgreSQL")
install.packages("ggplot2")
install.packages("plyr")
source('~/.active-rstudio-document')
View(df)
?colSums
?mean
?lm
lm
mean
colSums
setwd("C:/Users/jfuchs/r_wdir/2015-07_capstone/Capstone")
?tm_map
# Some material sourced from http://onepager.togaware.com/
# Hands-On Data Science with R - Text Mining
# Graham.Williams@togaware.com
# 5th November 2014
setwd("~/r_wdir/2015-07_capstone/Capstone")
library(tm)
library(SnowballC)
library(wordcloud)
library(rJava)
library(qdap)
library(magrittr) # for piping %>%
library(ggplot2)
library(stringr)
library(dplyr)
# Create the corpus
dirName <- file.path(".", "samples")
docs <- Corpus(DirSource(dirName))
?tm_map
docs <- tm_map(docs, content_transformer(tolower))          # Convert to lowercase
load("C:/Users/jfuchs/r_wdir/2015-07_capstone/Capstone/.RData")
install.packages("RWeka")
library(RWeka)
?NGramTokenizer
?DocumentTermMatrix
dtm
inspect(dtm[1:3, 1:10])
BigramTokenizer <- function (x) {
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
BigramTokenizer("abcdefgh")
BigramTokenizer("a", "b","c","d","e","f","g","h")
BigramTokenizer(c("a", "b","c","d","e","f","g","h"))
temp <- BigramTokenizer(c("a", "b","c","d","e","f","g","h"))
class(c("a", "b","c","d","e","f","g","h"))
length(class(c("a", "b","c","d","e","f","g","h")))
temp <- BigramTokenizer(c("a","b","c","d","e","f","g","h"))
BigramTokenizer <- function (x) {
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm2 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
tdm2
inspect(tdm2[1:10,1:3])
inspect(tdm2[100:110,1:3])
inspect(tdm2[1000:1010,1:3])
inspect(tdm2[10000:10010,1:3])
inspect(tdm2[100000:100010,1:3])
inspect(tdm2[1000000:1000010,1:3])
table(inspect(tdm2[1000000:1000010,1:3]))
inspect(tdm2[1000000:1000020,1])
inspect(tdm2[1000000:1000020,2])
inspect(tdm2[1000000:1000020,3])
inspect(tdm2[1000000:1000020,1])
inspect(tdm2[1000000:1000020,2])
TrigramTokenizer <- function (x) {
NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
tdm3 <- TermDocumentMatrix(docs, control = list(tokenize = TrigramTokenizer))
inspect(tdm3[1000000:1000020,2])
inspect(tdm3[1000000:1000020,1])
?stopwords
stopwords("english")
?file
sampleLines <- function(oldFileName, percentage, newFileName){
# Open a new connection to the file
con <- file(oldFileName, "rb")
# Make sure the new file is blank
if (file.exists(newFileName)) {
file.remove(newFileName)
}
file.create(newFileName)
numberOfLines <- 0
# Read each line and flip a coin to decide whether to include it
while (length(line <- readLines(con, 1, skipNul = TRUE)) > 0) {
if (rbinom(1, 1, percentage) == 1) {
cat(line, file = newFileName, append = TRUE, sep = "\n")
}
numberOfLines <- numberOfLines + 1
}
close(con)
print(numberOfLines)
}
sampleLines(oldFileName = "Coursera-SwiftKey/final/en_US/en_US.blogs.txt",
percentage = 0.05,
newFileName = "sample_en_US.blogs.txt")
setwd("C:/Users/jfuchs/r_wdir/2015-07_capstone/Capstone")
sampleLines(oldFileName = "Coursera-SwiftKey/final/en_US/en_US.blogs.txt",
percentage = 0.05,
newFileName = "sample_en_US.blogs.txt")
sampleLines(oldFileName = "Coursera-SwiftKey/final/en_US/en_US.blogs.txt",
percentage = 0.05,
newFileName = "sample_en_US.blogs.txt")
sampleLines(oldFileName = "Coursera-SwiftKey/final/en_US/en_US.twitter.txt",
percentage = 0.05,
newFileName = "sample_en_US.twitter.txt")
sampleLines(oldFileName = "Coursera-SwiftKey/final/en_US/en_US.news.txt",
percentage = 0.05,
newFileName = "sample_en_US.news.txt")
setwd("C:/Users/jfuchs/r_wdir/2015-07_capstone/Capstone")
library(tm)
library(SnowballC)
library(wordcloud)
library(rJava)
library(qdap)
library(magrittr) # for piping %>%
library(ggplot2)
library(stringr)
library(dplyr)
library(RWeka)
dirName <- file.path(".", "samples")
docs <- Corpus(DirSource(dirName))
docs
inspect(docs)
docs <- tm_map(docs, content_transformer(tolower))          # Convert to lowercase
docs[1][[1]][[1]][1]
docs <- tm_map(docs, removeNumbers)                         # Remove numbers
docs <- tm_map(docs, removePunctuation)                     # Remove punctuation
docs <- tm_map(docs, stripWhitespace)                       # Remove extra white space
docs[1][[1]][[1]][1]
docs[2][[1]][[1]][1]
docs[3][[1]][[1]][1]
dtm <- DocumentTermMatrix(docs)
docsNoStops <- tm_map(docs, removeWords, stopwords("english"))     # Remove stop words
docsStemmed <- tm_map(docs, stemDocument)                          # Stem words
dtm
freq <- colSums(as.matrix(dtm))
ord <- order(freq)
freq[tail(ord, 10)]
dtmSparse <- removeSparseTerms(dtm, 0.05)
dtmSparse
freq[head(ord, 10)]
freqSparse <- colSums(as.matrix(dtmSparse))
ordSparse <- order(freqSparse)
freqSparse[tail(ordSparse, 10)]
freqSparse[head(ordSparse, 10)]
set.seed(123)
wordcloud(names(freqSparse), freqSparse, min.freq = 10000,
colors = brewer.pal(6, "Dark2"), scale = c(3, 0.1))
wordcloud(names(freqSparse), freqSparse, min.freq = 10000,
colors = brewer.pal(6, "Dark2"), scale = c(5, 1))
words <- dtmSparse %>%
as.matrix %>%
colnames %>%
(function(x) x[nchar(x) < 20])
summary(nchar(words))
data.frame(nletters=nchar(words)) %>%
ggplot(aes(x=nletters)) +
geom_histogram(binwidth=1) +
geom_vline(xintercept=mean(nchar(words)),
colour="green", size=1, alpha=.5) +
labs(x="Number of Letters", y="Number of Words")
words %>%
str_split("") %>%
sapply(function(x) x[-1]) %>%
unlist %>%
dist_tab %>%
mutate(Letter=factor(toupper(interval),
levels=toupper(interval[order(freq)]))) %>%
ggplot(aes(Letter, weight=percent)) +
geom_bar() +
coord_flip() +
ylab("Proportion") +
scale_y_continuous(breaks=seq(0, 12, 2),
label=function(x) paste0(x, "%"),
expand=c(0,0), limits=c(0,12))
BigramTokenizer <- function (x) {
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
TrigramTokenizer <- function (x) {
NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
tdm2 <- TermDocumentMatrix(docs, control = list(tokenize = BigramTokenizer))
inspect(tdm2[1000000:1000020,2])
docs
inspect(docs)
tdm3 <- TermDocumentMatrix(docs, control = list(tokenize = TrigramTokenizer))
inspect(tdm3[1000000:1000020,1])
?TermDocumentMatrix
tdm2$i[1:10]
inspect(tdm2)
tdm2$i[1:10]
?sparseMatrix
tdm2$Terms
tdm2$dimnames$Terms
head(tdm2$dimnames$Terms)
tdm2
tdm2[1]
str(tdm2)
dtmSparse <- removeSparseTerms(dtm, 0.4)
freqSparse <- colSums(as.matrix(dtmSparse))
ordSparse <- order(freqSparse)
freqSparse[tail(ordSparse, 10)]
freqSparse[head(ordSparse, 10)]
dtmSparse <- removeSparseTerms(dtm, 0.05)
freqSparse <- colSums(as.matrix(dtmSparse))
ordSparse <- order(freqSparse)
freqSparse[tail(ordSparse, 10)]
freqSparse[head(ordSparse, 10)]
inspect(tdm2[1000000:1000020,2])
inspect(tdm3[1000000:1000020,1])
inspect(tdm2[1000000:1000020,])
vOrder2 <- order(tdm2$v)
tail(tdm2$v)
head(tdm2$v,50)
head(vOrder2)
head(vOrder2,50)
tail(vOrder2,50)
tdm2$dimnames$Terms[tail(vOrder2, 15)]
tail(vOrder2,15)
tdm2$dimnames$Terms[411511]
tail(vOrder2,15)
tdm2$dimnames$Terms[tail(vOrder2, 15)]
tail(tdm2$dimnames$Terms)
tdm2$dimnames$Terms[1]
tdm2$dimnames$Terms[500]
tdm2$dimnames$Terms[1000]
tdm2[1000,]
tdm2[i=500]
tdm2$i[1]
tdm2$i[500]
findFreqTerms(tdm2, 100)
findFreqTerms(tdm2, 1000)
findFreqTerms(tdm2, 10000)
findFreqTerms(tdm2, 5000)
findFreqTerms(tdm3, 5000)
findFreqTerms(tdm3, 1000)
findFreqTerms(tdm3, 500)
findFreqTerms(tdm2, 5000)
dtmSparse
docs
docs$sample_en_US.blogs.txt
inspect(docs)
dtmSparse
summary(dtmSparse)
str(dtmSparse)
termFreq <- colSums(as.matrix(tdm2))
termFreq
termFreq <- rowSums(as.matrix(tdm2))
termFreq
term2Freq <- rowSums(as.matrix(tdm2))
ord2Terms <- order(term2Freq)
term2Freq <- rowSums(as.matrix(tdm2))
ord2Terms <- order(term2Freq)
term2Freq[tail(ord2Terms, 10)]
term3Freq <- rowSums(as.matrix(tdm3))
ord3Terms <- order(term3Freq)
term3Freq[tail(ord3Terms, 10)]
set.seed(123)
wordcloud(names(term2Freq), term2Freq, min.freq = 10000,
colors = brewer.pal(6, "Dark2"), scale = c(5, 1))
wordcloud(names(term2Freq), term2Freq, min.freq = 5000,
colors = brewer.pal(6, "Dark2"), scale = c(5, 1))
wordcloud(names(term2Freq), term2Freq, min.freq = 1000,
colors = brewer.pal(6, "Dark2"), scale = c(5, 1))
wordcloud(names(term2Freq), term2Freq, min.freq = 1000, max.words = 100
colors = brewer.pal(6, "Dark2"), scale = c(5, 1))
wordcloud(names(term2Freq), term2Freq, min.freq = 1000, max.words = 100,
colors = brewer.pal(6, "Dark2"), scale = c(5, 1))
wordcloud(names(term3Freq), term3Freq, min.freq = 1000, max.words = 100,
colors = brewer.pal(6, "Dark2"), scale = c(5, 1))
wordcloud(names(term3Freq), term3Freq, min.freq = 500, max.words = 100,
colors = brewer.pal(6, "Dark2"), scale = c(5, 1))
wordcloud(names(term3Freq), term3Freq, min.freq = 500, max.words = 100,
colors = brewer.pal(6, "Dark2"), scale = c(3, 1))

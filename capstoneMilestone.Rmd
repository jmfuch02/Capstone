---
title: "Capstone Milestone Report"
author: "Jason Fuchs"
date: "July 2015"
output: html_document
---

# Introduction

# Process

First, I downloaded the three text files and wrote a sampling function, shown in the appendix, to make them smaller and more manageable. My function can take any percentage input. After some trial and error, I chose 10%, as this seemed to make the processing speed manageable.

```{r, echo=FALSE, eval=TRUE, warning=FALSE, error=FALSE, cache=TRUE, message=FALSE}
# Some material sourced from http://onepager.togaware.com/
# Hands-On Data Science with R - Text Mining
# Graham.Williams@togaware.com
# 5th November 2014

setwd("~/r_wdir/2015-07_capstone/Capstone")
library(tm)
library(SnowballC)
library(wordcloud)
library(rJava)
library(qdap)
library(magrittr) # for piping %>%
library(ggplot2)
library(stringr)
library(dplyr)

# Create the corpus
dirName <- file.path(".", "samples")
docs <- Corpus(DirSource(dirName))

```

It was important here to keep the samples static so that I wouldn't have to recreate them every time.

Once I had the samples, I converted all three into a Corpus (a collection of documents) using the tm library, and then I used the 'tm_map' function to clean them. You can see the transformations done to the corpus below.

```{r, echo=TRUE, eval=TRUE}
docs <- tm_map(docs, content_transformer(tolower))          # Convert to lowercase
docs <- tm_map(docs, removeNumbers)                         # Remove numbers
docs <- tm_map(docs, removePunctuation)                     # Remove punctuation
docs <- tm_map(docs, removeWords, stopwords("english"))     # Remove stop words
docs <- tm_map(docs, stripWhitespace)                       # Remove extra white space
docs <- tm_map(docs, stemDocument)                          # Stem words
```

```{r, echo=FALSE, eval=TRUE, warning=FALSE, error=FALSE, cache=TRUE, message=FALSE}
# Create the document term matrix
dtm <- DocumentTermMatrix(docs)

# Look at 10 most frequent words
freq <- colSums(as.matrix(dtm))
ord <- order(freq)

# Get rid of sparse terms
dtms <- removeSparseTerms(dtm, 0.05)
```

The most frequently-appearing words can be seen in the cloud below.

```{r, echo=FALSE, eval=TRUE, warning=FALSE, error=FALSE, cache=TRUE, message=FALSE}
# Make a word cloud
set.seed(123)
wordcloud(names(freq), freq, min.freq = 10000,
          colors = brewer.pal(6, "Dark2"), scale = c(3, 0.1))

```

```{r, echo=FALSE, eval=TRUE, warning=FALSE, error=FALSE, cache=TRUE, message=FALSE}

# Do some quantitative analysis
#words <- colnames(as.matrix(dtm))[nchar(colnames(as.matrix(dtm))) < 20]
words <- dtm %>%
    as.matrix %>%
    colnames %>%
    (function(x) x[nchar(x) < 20])

summary(nchar(words))
dist_tab(nchar(words))

# Create a histrogram of the word lengths
data.frame(nletters=nchar(words)) %>%
    ggplot(aes(x=nletters)) +
    geom_histogram(binwidth=1) +
    geom_vline(xintercept=mean(nchar(words)),
               colour="green", size=1, alpha=.5) +
    labs(x="Number of Letters", y="Number of Words")

# Show most common letters
words %>%
    str_split("") %>%
    sapply(function(x) x[-1]) %>%
    unlist %>%
    dist_tab %>%
    mutate(Letter=factor(toupper(interval),
                         levels=toupper(interval[order(freq)]))) %>%
    ggplot(aes(Letter, weight=percent)) +
    geom_bar() +
    coord_flip() +
    ylab("Proportion") +
    scale_y_continuous(breaks=seq(0, 12, 2),
                       label=function(x) paste0(x, "%"),
                       expand=c(0,0), limits=c(0,12))

```


```{r}
freq[tail(ord, 10)]
```


# Code Appendix

```{r, echo=TRUE, eval=FALSE}

sampleLines <- function(oldFileName, percentage, newFileName){

    # Open a new connection to the file
    con <- file(oldFileName, "rb")
    
    # Make sure the new file is blank
    if (file.exists(newFileName)) {
        file.remove(newFileName)
    }
    
    file.create(newFileName)
    numberOfLines <- 0
    
    # Read each line and flip a coin to decide whether to include it
    while (length(line <- readLines(con, 1, skipNul = TRUE)) > 0) { 
        
        if (rbinom(1, 1, percentage) == 1) {
            cat(line, file = newFileName, append = TRUE, sep = "\n")
        }
    
        numberOfLines <- numberOfLines + 1    
    
    }
    
    close(con)
    print(numberOfLines)
}

```

